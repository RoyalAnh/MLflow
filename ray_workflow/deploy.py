import os
import sys
import ray
from ray import serve
import mlflow
from serve_inference import InferenceModel
import tempfile
import zipfile
import shutil

def get_checkpoint_from_mlflow(run_id, artifact_path="checkpoints"):
    client = mlflow.tracking.MlflowClient()
    # Táº£i artifact checkpoint (cÃ³ thá»ƒ lÃ  folder hoáº·c file zip)
    artifacts = client.list_artifacts(run_id, path=artifact_path)
    if len(artifacts) == 1 and artifacts[0].path.endswith(".zip"):
        # Náº¿u checkpoint lÃ  file zip
        zip_artifact_path = artifacts[0].path
        tmpdir = tempfile.mkdtemp()
        zip_local = client.download_artifacts(run_id, zip_artifact_path, tmpdir)
        unzip_dir = os.path.join(tmpdir, "checkpoint_unzipped")
        os.makedirs(unzip_dir, exist_ok=True)
        with zipfile.ZipFile(zip_local, "r") as zip_ref:
            zip_ref.extractall(unzip_dir)
        # Tráº£ vá» Ä‘Ãºng thÆ° má»¥c chá»©a checkpoint (cÃ³ thá»ƒ pháº£i Ä‘iá»u chá»‰nh náº¿u bÃªn trong zip cÃ³ 1 thÆ° má»¥c)
        for root, dirs, files in os.walk(unzip_dir):
            if "params.json" in files or "rllib_checkpoint.json" in files:
                return root
        # Náº¿u khÃ´ng tÃ¬m tháº¥y file params.json, tráº£ vá» luÃ´n thÆ° má»¥c unzip
        return unzip_dir
    else:
        # Náº¿u checkpoint lÃ  thÆ° má»¥c artifact
        local_path = client.download_artifacts(run_id, artifact_path)
        return local_path

if __name__ == "__main__":
    # Nháº­n run_id (tá»« dÃ²ng lá»‡nh hoáº·c biáº¿n mÃ´i trÆ°á»ng)
    if len(sys.argv) > 1:
        run_id = sys.argv[1]
    else:
        run_id = os.environ.get("RUN_ID")
    if not run_id:
        raise RuntimeError("Báº¡n cáº§n truyá»n run_id qua dÃ²ng lá»‡nh hoáº·c biáº¿n mÃ´i trÆ°á»ng RUN_ID")
    checkpoint_path = get_checkpoint_from_mlflow(run_id)
    print(f"Checkpoint path dÃ¹ng Ä‘á»ƒ deploy: {checkpoint_path}")
    ray.init(ignore_reinit_error=True)
    serve.start(detached=True)
    serve.run(InferenceModel.bind(checkpoint_path=checkpoint_path), route_prefix="/predict")
    print("ğŸš€ Inference endpoint ready at: http://localhost:8000/predict")
    import time
    print("ğŸ”„ Press Ctrl+C to stop.")
    while True:
        time.sleep(1)
